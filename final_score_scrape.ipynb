{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f8c86f2e-21b1-43c2-850f-ee9334957edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import os\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9b4b9556-79cc-4b5a-82db-5f430cf3c752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2019, 2020, 2021, 2022, 2023, 2024]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seasons = list(range(2019,2025))\n",
    "seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f0e3967-173a-411e-af55-70d4735a1192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the browser\n",
    "executable_path = {'executable_path':\"C:\\Program Files (x86)\\msedgedriver.exe\"}\n",
    "browser = Browser('edge', **executable_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a1f60598-3ad4-4739-b321-c32a84e740d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_season_info(soup):\n",
    "    nav = soup.find('div', id='bottom_nav_container')\n",
    "    hrefs = [a[\"href\"] for a in nav.find_all(\"a\")]\n",
    "    season = hrefs[1].split()[0].split('_')[0].split('/')[-1]\n",
    "    return season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2fa55353-887a-4af9-9f5e-a04270f9155b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 / 225\n",
      "200 / 225\n",
      "300 / 225\n",
      "400 / 209\n",
      "500 / 209\n",
      "600 / 231\n",
      "700 / 231\n",
      "800 / 163\n",
      "900 / 163\n",
      "1000 / 229\n",
      "1100 / 229\n",
      "1200 / 129\n",
      "1300 / 38\n",
      "1400 / 102\n",
      "1500 / 222\n",
      "1600 / 222\n",
      "1700 / 222\n",
      "1800 / 222\n",
      "1900 / 222\n",
      "2000 / 222\n",
      "2100 / 163\n",
      "2200 / 163\n",
      "2300 / 229\n",
      "2400 / 229\n",
      "2500 / 121\n",
      "2600 / 34\n",
      "2700 / 219\n",
      "2800 / 219\n",
      "2900 / 219\n",
      "3000 / 208\n",
      "3100 / 208\n"
     ]
    }
   ],
   "source": [
    "# Check to see if the page has loaded\n",
    "def page_loaded(browser):\n",
    "    return browser.is_element_present_by_id('line_score', wait_time=5)\n",
    "\n",
    "games = []\n",
    "\n",
    "for season in seasons:\n",
    "    # Visit the website for scraping\n",
    "    url = f\"https://www.basketball-reference.com/leagues/NBA_{season}_games.html\"\n",
    "    browser.visit(url)\n",
    "    time.sleep(2)\n",
    "    print(f'Gather data for {season}')\n",
    "    \n",
    "    # Check if the page has loaded\n",
    "    if not page_loaded(browser):\n",
    "        # If not, reload the page and wait\n",
    "        browser.reload()\n",
    "        time.sleep(3)\n",
    "\n",
    "    # Create a BeautifulSoup object\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    table = soup.find('div', id='content')\n",
    "\n",
    "    if table is not None:\n",
    "        month_filter = table.find('div', class_='filter')\n",
    "        links = month_filter.find_all(\"a\")\n",
    "        hrefs = [link.get(\"href\") for link in links]\n",
    "        standing_pages = [f\"https://www.basketball-reference.com{link}\" for link in hrefs[:-1]]\n",
    "        \n",
    "        for page in standing_pages:\n",
    "            browser.visit(page)\n",
    "            time.sleep(2) \n",
    "            \n",
    "            # Create a BeautifulSoup object\n",
    "            html = browser.html\n",
    "            soup_page = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "            links = soup_page.find_all(\"a\")\n",
    "            hrefs = [link.get(\"href\") for link in links]\n",
    "            \n",
    "             # Filer out 2024 links\n",
    "            box_scores = [link for link in hrefs if link and \"boxscore\" in link and \".html\" in link and '2024' not in link]\n",
    "            box_scores = [f\"https://www.basketball-reference.com{score}\" for score in box_scores]\n",
    "\n",
    "            base_cols = None\n",
    "            for box_score in box_scores:\n",
    "                browser.visit(box_score)\n",
    "                time.sleep(2)\n",
    "                \n",
    "                html=browser.html\n",
    "                soup_box = BeautifulSoup(html, 'html.parser')\n",
    "                \n",
    "                score_table = soup_box.find('table', id='line_score')\n",
    "                \n",
    "                # Cheack to see if score_table is None\n",
    "                if score_table is None:\n",
    "                    browser.reload()\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                    # Re-create the BeautifulSoup object\n",
    "                    html=browser.html\n",
    "                    soup_box = BeautifulSoup(html, 'html.parser')\n",
    "                    score_table = soup_box.find('table', id='line_score')\n",
    "                \n",
    "                tbody = score_table.find('tbody')\n",
    "                rows = tbody.find_all('tr')\n",
    "\n",
    "                line_score = []\n",
    "                for row in rows:\n",
    "                    # Get team name\n",
    "                    team = row.find('th', class_='center').text\n",
    "                    columns = row.find_all('td')\n",
    "\n",
    "                    # Create line score dictionary\n",
    "                    if(columns !=[]):\n",
    "                        total = columns[4].text\n",
    "\n",
    "                    # Create dictionary for dataframe later\n",
    "                    line_score_dict = { \"team\": team,\n",
    "                                        \"total\": total\n",
    "                    }\n",
    "\n",
    "                    # Add dictionary to array\n",
    "                    line_score.append(line_score_dict)\n",
    "\n",
    "                # Create Data frame\n",
    "                score_df = pd.DataFrame(line_score)\n",
    "                teams = [score[\"team\"] for score in line_score]\n",
    "\n",
    "                summaries = []\n",
    "                for team in teams:\n",
    "                    # Convert html table into pandas dataframe\n",
    "                    basic = pd.read_html(str(soup_box), attrs={\"id\": f\"box-{team}-game-basic\"}, index_col=0)[0]\n",
    "                    advanced = pd.read_html(str(soup_box), attrs={\"id\": f\"box-{team}-game-advanced\"}, index_col=0)[0]\n",
    "\n",
    "                    # Convert all columns with number to numeric for dataframes\n",
    "                    basic = basic.apply(pd.to_numeric, errors=\"coerce\")\n",
    "                    advanced = advanced.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "                    # Remove the row that contains heards within the dataframe\n",
    "                    advanced = advanced.drop('Reserves')\n",
    "                    basic = basic.drop('Reserves')\n",
    "\n",
    "                    # Get headers for basic and advanced stats\n",
    "                    advanced_columns = []\n",
    "                    basic_columns = []\n",
    "                    for i in range(len(advanced.columns)):\n",
    "                        advanced_columns.append(advanced.columns[i][1])\n",
    "\n",
    "                    for i in range(len(basic.columns)):\n",
    "                        basic_columns.append(basic.columns[i][1])\n",
    "\n",
    "                    advanced.columns = advanced_columns\n",
    "                    basic.columns = basic_columns\n",
    "\n",
    "                    # Totals and Maxes\n",
    "                    totals = pd.concat([basic.iloc[-1, :], advanced.iloc[-1, :]])\n",
    "                    totals.index = totals.index.str.lower()\n",
    "\n",
    "                    maxes = pd.concat([basic.iloc[:-1, :].max(), advanced.iloc[:-1, :].max()])\n",
    "                    maxes.index = maxes.index.str.lower() + \"_max\"\n",
    "\n",
    "                    summary = pd.concat([totals, maxes])\n",
    "\n",
    "                    # Create common columns that are found for all teams\n",
    "                    if base_cols is None:\n",
    "                        base_cols = list(summary.index.drop_duplicates(keep=\"first\"))\n",
    "                        base_cols = [b for b in base_cols if \"bpm\" not in b]\n",
    "\n",
    "                    summary = summary[base_cols]\n",
    "                    summaries.append(summary)\n",
    "\n",
    "                summary = pd.concat(summaries, axis=1).T\n",
    "                game = pd.concat([summary, score_df], axis=1)\n",
    "                game[\"home\"] = [0, 1]\n",
    "\n",
    "                game_opp = game.iloc[::-1].reset_index()\n",
    "                game_opp.columns += \"_opp\"\n",
    "\n",
    "                # Merge both home and away team data together\n",
    "                full_game = pd.concat([game, game_opp], axis=1)\n",
    "\n",
    "                # Add the season the game was played in\n",
    "                full_game[\"season\"] = read_season_info(soup_box)\n",
    "\n",
    "                # Add date to dataframe\n",
    "                full_game[\"date\"] = box_score.split('/')[-1][:8]\n",
    "                full_game[\"date\"] = pd.to_datetime(full_game[\"date\"], format=\"%Y%m%d\")\n",
    "\n",
    "                # Specify who won the game\n",
    "                full_game[\"won\"] = full_game[\"total\"] > full_game[\"total_opp\"]\n",
    "\n",
    "                games.append(full_game)\n",
    "\n",
    "                if len(games) % 100 == 0:\n",
    "                    print(f\"{len(games)} / {len(box_scores)}\")\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8928eeae-6367-4f71-9d61-358f95a94b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "games_df = pd.concat(games, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2d45a234-244b-4a36-a3bc-777ec44cd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "games_df.to_csv(\"nba_games.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381cc7c4-1d43-4536-bc93-e077c049e5d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
